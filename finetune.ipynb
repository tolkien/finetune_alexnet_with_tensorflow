{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Script to finetune AlexNet using Tensorflow.\n",
    "\n",
    "With this script you can finetune AlexNet as provided in the alexnet.py\n",
    "class on any given dataset. Specify the configuration settings at the\n",
    "beginning according to your problem.\n",
    "This script was written for TensorFlow >= version 1.2rc0 and comes with a blog\n",
    "post, which you can find here:\n",
    "\n",
    "https://kratzert.github.io/2017/02/24/finetuning-alexnet-with-tensorflow.html\n",
    "\n",
    "Author: Frederik Kratzert\n",
    "contact: f.kratzert(at)gmail.com\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from alexnet import AlexNet\n",
    "from datagenerator import ImageDataGenerator\n",
    "from datetime import datetime\n",
    "\n",
    "\"\"\"\n",
    "Configuration Part.\n",
    "\"\"\"\n",
    "\n",
    "# Path to the textfiles for the trainings and validation set\n",
    "train_file = '/home/tolkien/misc/tf/AlexNet/food/converted/train.txt'\n",
    "val_file = '/home/tolkien/misc/tf/AlexNet/food/converted/val_1.txt'\n",
    "\n",
    "# Learning params\n",
    "learning_rate = 0.000004\n",
    "num_epochs = 10\n",
    "batch_size = 17\n",
    "\n",
    "# Network params\n",
    "dropout_rate = 0.5\n",
    "num_classes = 4\n",
    "train_layers = ['fc8']\n",
    "\n",
    "\n",
    "# How often we want to write the tf.summary data to disk\n",
    "display_step = 20\n",
    "\n",
    "# Path for tf.summary.FileWriter and to store model checkpoints\n",
    "filewriter_path = \"/tmp/finetune_alexnet/tensorboard\"\n",
    "checkpoint_path = \"/tmp/finetune_alexnet/checkpoints\"\n",
    "if not os.path.isdir(\"/tmp/finetune_alexnet/\"):\n",
    "    os.mkdir(\"/tmp/finetune_alexnet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from caffe_classes import class_names\n",
    "print(len(class_names))\n",
    "print(class_names[1000], class_names[1001], class_names[1002], class_names[1003])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc8/weights:0/gradient is illegal; using fc8/weights_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc8/biases:0/gradient is illegal; using fc8/biases_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc8/weights:0 is illegal; using fc8/weights_0 instead.\n",
      "INFO:tensorflow:Summary name fc8/biases:0 is illegal; using fc8/biases_0 instead.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main Part of the finetuning Script.\n",
    "\"\"\"\n",
    "\n",
    "# Create parent path if it doesn't exist\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    os.mkdir(checkpoint_path)\n",
    "\n",
    "# Place data loading and preprocessing on the cpu\n",
    "with tf.device('/cpu:0'):\n",
    "    tr_data = ImageDataGenerator(train_file,\n",
    "                                 mode='training',\n",
    "                                 batch_size=batch_size,\n",
    "                                 num_classes=num_classes,\n",
    "                                 shuffle=True)\n",
    "    val_data = ImageDataGenerator(val_file,\n",
    "                                  mode='inference',\n",
    "                                  batch_size=batch_size,\n",
    "                                  num_classes=num_classes,\n",
    "                                  shuffle=False)\n",
    "\n",
    "    # create an reinitializable iterator given the dataset structure\n",
    "    iterator = tf.data.Iterator.from_structure(tr_data.data.output_types,\n",
    "                                       tr_data.data.output_shapes)\n",
    "    next_batch = iterator.get_next()\n",
    "\n",
    "# Ops for initializing the two different iterators\n",
    "training_init_op = iterator.make_initializer(tr_data.data)\n",
    "validation_init_op = iterator.make_initializer(val_data.data)\n",
    "\n",
    "# TF placeholder for graph input and output\n",
    "x = tf.placeholder(tf.float32, [batch_size, 227, 227, 3])\n",
    "y = tf.placeholder(tf.float32, [batch_size, num_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Initialize model\n",
    "model = AlexNet(x, keep_prob, num_classes, train_layers)\n",
    "\n",
    "# Link variable to model output\n",
    "score = model.fc8\n",
    "\n",
    "# List of trainable variables of the layers we want to train\n",
    "var_list = [v for v in tf.trainable_variables() if v.name.split('/')[0] in train_layers]\n",
    "\n",
    "# Op for calculating the loss\n",
    "with tf.name_scope(\"cross_ent\"):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=score,\n",
    "                                                                  labels=y))\n",
    "\n",
    "# Train op\n",
    "with tf.name_scope(\"train\"):\n",
    "    # Get gradients of all trainable variables\n",
    "    gradients = tf.gradients(loss, var_list)\n",
    "    gradients = list(zip(gradients, var_list))\n",
    "\n",
    "    # Create optimizer and apply gradient descent to the trainable variables\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars=gradients)\n",
    "\n",
    "# Add gradients to summary\n",
    "for gradient, var in gradients:\n",
    "    tf.summary.histogram(var.name + '/gradient', gradient)\n",
    "\n",
    "# Add the variables we train to the summary\n",
    "for var in var_list:\n",
    "    tf.summary.histogram(var.name, var)\n",
    "\n",
    "# Add the loss to summary\n",
    "tf.summary.scalar('cross_entropy', loss)\n",
    "\n",
    "\n",
    "# Evaluation op: Accuracy of the model\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(score, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Add the accuracy to the summary\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# Merge all summaries together\n",
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "# Initialize the FileWriter\n",
    "writer = tf.summary.FileWriter(filewriter_path)\n",
    "\n",
    "# Initialize an saver for store model checkpoints\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Get the number of training/validation steps per epoch\n",
    "train_batches_per_epoch = int(np.floor(tr_data.data_size/batch_size))\n",
    "val_batches_per_epoch = int(np.floor(val_data.data_size / batch_size))\n",
    "\n",
    "# Start Tensorflow session\n",
    "def train():\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # Initialize all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Add the model graph to TensorBoard\n",
    "    writer.add_graph(sess.graph)\n",
    "\n",
    "    # Load the pretrained weights into the non-trainable layer\n",
    "    model.load_initial_weights(sess)\n",
    "\n",
    "    print(\"{} Start training...\".format(datetime.now()))\n",
    "    print(\"{} Open Tensorboard at --logdir {}\".format(datetime.now(),\n",
    "                                                      filewriter_path))\n",
    "\n",
    "    # Loop over number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        print(\"{} Epoch number: {}\".format(datetime.now(), epoch+1))\n",
    "\n",
    "        # Initialize iterator with the training dataset\n",
    "        sess.run(training_init_op)\n",
    "\n",
    "        for step in range(train_batches_per_epoch):\n",
    "\n",
    "            # get next batch of data\n",
    "            img_batch, label_batch = sess.run(next_batch)\n",
    "\n",
    "            # And run the training op\n",
    "            sess.run(train_op, feed_dict={x: img_batch,\n",
    "                                          y: label_batch,\n",
    "                                          keep_prob: dropout_rate})\n",
    "\n",
    "            # Generate summary with the current batch of data and write to file\n",
    "            if step % display_step == 0:\n",
    "                s = sess.run(merged_summary, feed_dict={x: img_batch,\n",
    "                                                        y: label_batch,\n",
    "                                                        keep_prob: 1.})\n",
    "\n",
    "                writer.add_summary(s, epoch*train_batches_per_epoch + step)\n",
    "\n",
    "        # Validate the model on the entire validation set\n",
    "        print(\"{} Start validation\".format(datetime.now()))\n",
    "        sess.run(validation_init_op)\n",
    "        test_acc = 0.\n",
    "        test_count = 0\n",
    "        for _ in range(val_batches_per_epoch):\n",
    "\n",
    "            img_batch, label_batch = sess.run(next_batch)\n",
    "            acc = sess.run(accuracy, feed_dict={x: img_batch,\n",
    "                                                y: label_batch,\n",
    "                                                keep_prob: 1.})\n",
    "            test_acc += acc\n",
    "            test_count += 1\n",
    "        test_acc /= test_count\n",
    "        print(\"{} Validation Accuracy = {:.4f}\".format(datetime.now(),\n",
    "                                                       test_acc))\n",
    "        print(\"{} Saving checkpoint of model...\".format(datetime.now()))\n",
    "\n",
    "        # save checkpoint of the model\n",
    "        checkpoint_name = os.path.join(checkpoint_path,\n",
    "                                       'model_epoch'+str(epoch+1)+'.ckpt')\n",
    "        save_path = saver.save(sess, checkpoint_name)\n",
    "\n",
    "        print(\"{} Model checkpoint saved at {}\".format(datetime.now(),\n",
    "                                                       checkpoint_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-21 08:33:33.838188 Start training...\n",
      "2018-04-21 08:33:33.838298 Open Tensorboard at --logdir /tmp/finetune_alexnet/tensorboard\n",
      "2018-04-21 08:33:33.838317 Epoch number: 1\n",
      "2018-04-21 08:34:37.511109 Start validation\n",
      "2018-04-21 08:34:37.921803 Validation Accuracy = 0.4118\n",
      "2018-04-21 08:34:37.921917 Saving checkpoint of model...\n",
      "2018-04-21 08:34:38.596862 Model checkpoint saved at /tmp/finetune_alexnet/checkpoints/model_epoch1.ckpt\n",
      "2018-04-21 08:34:38.596966 Epoch number: 2\n",
      "2018-04-21 08:35:41.826369 Start validation\n",
      "2018-04-21 08:35:41.951066 Validation Accuracy = 0.4118\n",
      "2018-04-21 08:35:41.951183 Saving checkpoint of model...\n",
      "2018-04-21 08:35:42.465805 Model checkpoint saved at /tmp/finetune_alexnet/checkpoints/model_epoch2.ckpt\n",
      "2018-04-21 08:35:42.465918 Epoch number: 3\n",
      "2018-04-21 08:36:45.644376 Start validation\n",
      "2018-04-21 08:36:45.767067 Validation Accuracy = 0.4118\n",
      "2018-04-21 08:36:45.767176 Saving checkpoint of model...\n",
      "2018-04-21 08:36:46.290160 Model checkpoint saved at /tmp/finetune_alexnet/checkpoints/model_epoch3.ckpt\n",
      "2018-04-21 08:36:46.290272 Epoch number: 4\n",
      "2018-04-21 08:37:49.238230 Start validation\n",
      "2018-04-21 08:37:49.361640 Validation Accuracy = 0.4118\n",
      "2018-04-21 08:37:49.361752 Saving checkpoint of model...\n",
      "2018-04-21 08:37:49.867681 Model checkpoint saved at /tmp/finetune_alexnet/checkpoints/model_epoch4.ckpt\n",
      "2018-04-21 08:37:49.867793 Epoch number: 5\n",
      "2018-04-21 08:38:53.117030 Start validation\n",
      "2018-04-21 08:38:53.241229 Validation Accuracy = 0.4118\n",
      "2018-04-21 08:38:53.241339 Saving checkpoint of model...\n",
      "2018-04-21 08:38:53.801025 Model checkpoint saved at /tmp/finetune_alexnet/checkpoints/model_epoch5.ckpt\n",
      "2018-04-21 08:38:53.801139 Epoch number: 6\n",
      "2018-04-21 08:39:56.884371 Start validation\n",
      "2018-04-21 08:39:57.008120 Validation Accuracy = 0.4118\n",
      "2018-04-21 08:39:57.008229 Saving checkpoint of model...\n",
      "2018-04-21 08:39:57.546692 Model checkpoint saved at /tmp/finetune_alexnet/checkpoints/model_epoch6.ckpt\n",
      "2018-04-21 08:39:57.546788 Epoch number: 7\n",
      "2018-04-21 08:41:00.637000 Start validation\n",
      "2018-04-21 08:41:00.762928 Validation Accuracy = 0.4118\n",
      "2018-04-21 08:41:00.763036 Saving checkpoint of model...\n",
      "2018-04-21 08:41:01.296641 Model checkpoint saved at /tmp/finetune_alexnet/checkpoints/model_epoch7.ckpt\n",
      "2018-04-21 08:41:01.296740 Epoch number: 8\n",
      "2018-04-21 08:42:04.491890 Start validation\n",
      "2018-04-21 08:42:04.617947 Validation Accuracy = 0.4118\n",
      "2018-04-21 08:42:04.618058 Saving checkpoint of model...\n",
      "2018-04-21 08:42:05.154213 Model checkpoint saved at /tmp/finetune_alexnet/checkpoints/model_epoch8.ckpt\n",
      "2018-04-21 08:42:05.154307 Epoch number: 9\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
